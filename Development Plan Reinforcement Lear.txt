Development Plan: Reinforcement Learning Module Integration
1. Objective
To develop and integrate a complete Reinforcement Learning (RL) module for the Sappo Trading Bot. This plan outlines the creation of a custom trading environment, the implementation of a GRU-based agent, and the establishment of robust training and evaluation pipelines. The core of this plan is to utilize the fully pre-processed data, embracing a high-dimensional state representation for a comprehensive market view.

2. Core Strategy Update
Based on our analysis, we will proceed by using the complete pre-processed dataset.

State Representation: The agent's state will be a 2D Tensor with the shape (24, 529).

24 (Time Steps): Represents the 24-hour look-back window from the pre-processed data.

529 (Features): Composed of:

527 Market Features: The full set of features generated by the preprocessing.py script.

2 Portfolio Features: Current Position and Unrealized PnL.

3. Development Phases
Phase 1: Custom Environment Implementation (TradingEnv.py)
Task: Create a custom trading environment that complies with the gymnasium API.

Details:

Class TradingEnv(gymnasium.Env):

__init__(self, data_array, initial_balance=10000, transaction_cost=0.001):

Load the pre-processed (samples, 24, 527) NumPy array.

Define the action_space as Discrete(3) (0: Hold, 1: Buy, 2: Sell).

Define the observation_space as Box(low=-inf, high=inf, shape=(24, 529), dtype=np.float32).

step(self, action):

Execute the chosen action (Buy, Sell, Hold), updating the portfolio (balance, holdings, unrealized PnL).

Apply transaction costs for Buy/Sell actions.

Calculate the reward for the step using the agreed-upon formula: R_t = w_1*R_profit + w_2*R_sharpe - w_3*R_cost - w_4*R_mdd.

Advance to the next time step in the data array.

Return (observation, reward, terminated, truncated, info).

reset(self, *, seed=None, options=None):

Reset the portfolio, step counter, and all metrics.

Return the initial observation and info dictionary.

Outcome: A fully functional, self-contained TradingEnv class ready for agent interaction.

Phase 2: Agent and Model Architecture (agent.py)
Task: Implement the GRU-based Actor-Critic agent.

Details:

Framework: Utilize a standard RL library like stable-baselines3 with its PPO algorithm for robustness and efficiency.

Model Architecture: Define a custom policy network (ActorCriticPolicy).

Input Layer: Accepts tensors of shape (24, 529).

Feature Extractor:

One or two GRU layers to process the time-series data (e.g., 128 units).

A Dense layer with ReLU activation to process the GRU's output.

Output Heads:

Policy Head (Actor): A Dense layer outputting logits for the 3 actions.

Value Head (Critic): A Dense layer outputting a single value for the state valuation.

Outcome: An agent class capable of learning within the TradingEnv.

Phase 3: Training Pipeline (train.py)
Task: Create an executable script to manage the model training process.

Details:

Data Handling:

Load the final preprocessed_data_YYYYMMDD_HHMMSS.npy file.

Split the data chronologically: 70% for Training, 15% for Validation. The final 15% will be reserved for the final Test set.

Environment Setup:

Instantiate TradingEnv with the training dataset.

Wrap the environment with stable-baselines3 wrappers for monitoring.

Training Execution:

Instantiate the PPO agent with the custom GRU policy.

Use a Callback to periodically evaluate the agent on the validation set and save the model with the best performance (e.g., highest Sharpe Ratio).

Log all training metrics (reward, portfolio value, etc.) to TensorBoard for real-time monitoring.

Launch the training using agent.learn(total_timesteps=...).

Outcome: A script that trains the agent and saves the best-performing model checkpoint.

Phase 4: Evaluation and Reporting (evaluate.py)
Task: Create a script to evaluate the final, trained agent and generate a performance report.

Details:

Setup:

Load the best model saved from the training phase.

Instantiate TradingEnv with the unseen Test set (the final 15% of the data).

Backtesting:

Run the agent through the entire test environment in inference mode.

Record the portfolio value, trades, and other metrics at each step.

Reporting:

Calculate the final Key Performance Indicators (KPIs):

Cumulative Return

Sharpe Ratio

Maximum Drawdown (MDD)

Total Number of Trades

Generate visualizations, including a plot of the portfolio value vs. a "buy and hold" benchmark.

Outcome: An objective, data-driven report on the agent's trading performance on unseen data.



Final Development Plan: Integrated RL Trading Bot Dashboard
1. Project Objective
To develop a comprehensive, GUI-driven desktop application for the Sappo Trading Bot, managing the end-to-end workflow from initial data preprocessing to reinforcement learning model training and final evaluation. This plan outlines the integration of a user-friendly GUI with a powerful backend RL module, creating a seamless and efficient MLOps-style platform.

2. Core Strategy
The application will be built around a central GUI dashboard that guides the user through a structured, multi-stage process. The core strategy relies on:

Modular Design: Separating functionality into distinct modules (GUI, Preprocessing, Environment, Agent, Training, Evaluation).

High-Dimensional State: Utilizing the full, pre-processed (24, 529) dimensional data to provide the agent with a rich, comprehensive view of the market.

User-Centric Workflow: Enabling users to control and monitor complex processes through an intuitive graphical interface.

3. Phase 1: GUI Dashboard Implementation & Integration
Task: Develop a master GUI dashboard using a ttk.Notebook (Tab) structure to organize the workflow.

Details:

Tab 1: "Data Preprocessing"

This tab will house the existing functionality: selecting multiple raw CSV files, initiating the preprocessing pipeline, and viewing logs.

No major changes are required for this component.

Tab 2: "Training & Evaluation" (New)

This new tab will be the central hub for all RL-related activities. Its components are detailed below:

A. Data Selection:

Implement a single "Select Preprocessed Data (.npy)" button.

Rationale: The user selects one master .npy file. The application will then automatically and chronologically split the data in the backend (e.g., 70% Train, 15% Validation, 15% Test) to prevent data leakage.

B. Hyperparameter Configuration:

Create ttk.Entry widgets for key parameters:

Learning Rate (e.g., 0.0001)

Gamma (Discount Factor) (e.g., 0.99)

Reward Weights (w1_profit, w2_sharpe, w3_cost, w4_mdd).

C. Control & Execution:

Implement a "Start Training" button that initiates the training process in a separate thread to keep the GUI responsive. The best model based on validation performance will be saved automatically (e.g., best_model.zip).

Implement an "Evaluate Best Model" button that loads the saved model and runs it on the unseen test set, also in a separate thread.

D. Visualization & Reporting:

A text-based Log Area to display real-time training progress and final KPI reports.

An integrated Matplotlib Chart Area to automatically plot the portfolio value curve against a benchmark after evaluation is complete.

Outcome: A fully-featured GUI dashboard that serves as the command center for the entire project.

4. Phase 2: Custom Environment Implementation (TradingEnv.py)
Task: Create a custom gymnasium-compliant trading environment.

Details:

Class TradingEnv(gymnasium.Env):

__init__: Defines action_space (Discrete(3)) and observation_space (Box(shape=(24, 529))).

step(self, action): Executes the action, applies transaction costs, calculates the multi-component reward (R_profit, R_sharpe, R_cost, R_mdd), and returns the next state.

reset(self): Resets the environment to its initial state for a new episode.

Outcome: A reusable and robust environment for training and evaluating any RL agent.

5. Phase 3: Agent and Training Pipeline (agent.py, train.py)
Task: Implement the GRU-based agent and the script to manage the training process.

Details:

Agent Architecture:

Define a custom ActorCriticPolicy using a GRU layer for temporal feature extraction, followed by dense layers for the policy and value heads.

Training Script (train.py):

This script will be triggered by the "Start Training" button in the GUI.

It will receive the data path and hyperparameters from the GUI.

It will instantiate the TradingEnv, the PPO agent with the custom GRU policy, and start the learning process using stable-baselines3.

It will use a Callback to save the best-performing model based on validation metrics.

Outcome: A complete training pipeline that can be launched and configured directly from the application's user interface.

6. Phase 4: Evaluation Pipeline and Reporting (evaluate.py)
Task: Create the backend logic for the evaluation process.

Details:

Evaluation Script (evaluate.py):

This script will be triggered by the "Evaluate Best Model" button.

It loads the saved best_model.zip and runs it on the test portion of the data.

Reporting:

Calculates the final KPIs (Cumulative Return, Sharpe Ratio, MDD, etc.).

Generates the data for the portfolio performance chart.

The results (KPIs and chart data) will be passed back to the GUI for display in the "Training & Evaluation" tab.

Outcome: A seamless evaluation workflow that provides clear, actionable insights into the model's performance directly within the application.